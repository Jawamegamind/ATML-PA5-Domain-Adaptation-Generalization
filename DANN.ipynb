{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p1yVEbVN8-L"
      },
      "source": [
        "#### Installing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7biAx9XgN8-M",
        "outputId": "066d9a9c-204c-4f78-fcc7-d20cae40e9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znd15iolN8-N"
      },
      "source": [
        "#### Loading in the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovWTQfuRN8-N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torchvision.datasets import MNIST, USPS, SVHN\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.autograd import Function\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_t0hy4QOHp1",
        "outputId": "d754d609-85d7-4ee4-b9b2-c3fbc54d2103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/ATML_PA5'\n",
        "# Setting the path to this folder\n",
        "os.chdir(path)\n",
        "# Checking the current working directory\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKuOeHcwOMX3",
        "outputId": "5aa45bab-7ae7-4c15-8946-0bdd6cd58f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATML_PA5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQGrjHgSN8-O"
      },
      "source": [
        "#### Setting the device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t45yHeTlN8-O",
        "outputId": "72fc8ac9-38a0-47d1-8b12-6a013a698c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device is:  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"The device is: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFfIuslN8-P"
      },
      "source": [
        "## Loading in the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxHKMNxTN8-P"
      },
      "outputs": [],
      "source": [
        "# Function to create a subset\n",
        "def create_subset(dataset, subset_size, seed=42):\n",
        "    \"\"\"\n",
        "    Creates a random subset of the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): The original dataset.\n",
        "        subset_size (int): The number of samples in the subset.\n",
        "        seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Subset: A PyTorch Subset object.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    indices = random.sample(range(len(dataset)), subset_size)\n",
        "    return Subset(dataset, indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv-cj0xcN8-P"
      },
      "source": [
        "#### Office-31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuKECUjoN8-Q",
        "outputId": "110d1ef3-f0f6-4b0d-9349-d646515e9bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon_path: OFFICE31/amazon\n",
            "webcam_path: OFFICE31/webcam\n",
            "dslr_path: OFFICE31/dslr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon_loader size: 45\n",
            "webcam_loader size: 12\n",
            "dslr_loader size: 7\n"
          ]
        }
      ],
      "source": [
        "data_path = 'OFFICE31'\n",
        "amazon_path = os.path.join(data_path, 'amazon')\n",
        "webcam_path = os.path.join(data_path, 'webcam')\n",
        "dslr_path = os.path.join(data_path, 'dslr')\n",
        "\n",
        "print('amazon_path:', amazon_path)\n",
        "print('webcam_path:', webcam_path)\n",
        "print('dslr_path:', dslr_path)\n",
        "\n",
        "def load_data(root_path, domain, batch_size, phase):\n",
        "    transform_dict = {\n",
        "        'src': transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ]),\n",
        "        'tar': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ])}\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n",
        "    return data_loader\n",
        "\n",
        "amazon_loader = load_data(data_path, 'amazon', 64, 'src')\n",
        "webcam_loader = load_data(data_path, 'webcam', 64, 'tar')\n",
        "dslr_loader = load_data(data_path, 'dslr', 64, 'tar')\n",
        "\n",
        "# Checking the size of these data loaders\n",
        "print('amazon_loader size:', len(amazon_loader))\n",
        "print('webcam_loader size:', len(webcam_loader))\n",
        "print('dslr_loader size:', len(dslr_loader))\n",
        "\n",
        "# # Check the size of the first batch\n",
        "# amazon_data = next(iter(amazon_loader))\n",
        "# webcam_data = next(iter(webcam_loader))\n",
        "# dslr_data = next(iter(dslr_loader))\n",
        "\n",
        "# print('amazon_data size:', amazon_data[0].size())\n",
        "# print('webcam_data size:', webcam_data[0].size())\n",
        "# print('dslr_data size:', dslr_data[0].size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHS5-RJN8-Q"
      },
      "source": [
        "#### Digits Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMbkxHcDN8-Q",
        "outputId": "bc928c15-f103-457e-afa3-5fa2c7901482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n",
            "MNIST Train Size: 60000\n",
            "MNIST Test Size: 10000\n",
            "USPS Train Size: 7291\n",
            "USPS Test Size: 2007\n",
            "SVHN Train Size: 73257\n",
            "SVHN Test Size: 26032\n"
          ]
        }
      ],
      "source": [
        "# Define transformations for the datasets\n",
        "transform_mnist_usps = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                  # Resize to 224x224\n",
        "    transforms.Grayscale(num_output_channels=3),  # Ensure grayscale images (for USPS/MNIST) and convert to 3 channels\n",
        "    transforms.ToTensor(),                        # Convert to Tensor\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    # transforms.Normalize((0.5,), (0.5,))          # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "transform_svhn = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),                  # Resize to 224x224\n",
        "    transforms.ToTensor(),                        # Convert to Tensor\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    # transforms.Normalize((0.5,), (0.5,))          # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Loading in MNIST dataset\n",
        "mnist_train = MNIST(root='./data', train=True, download=True, transform=transform_mnist_usps)\n",
        "mnist_test = MNIST(root='./data', train=False, download=True, transform=transform_mnist_usps)\n",
        "\n",
        "# MNIST dataloaders\n",
        "mnist_train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "mnist_test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "# Loading in the USPS dataset\n",
        "usps_train = USPS(root='./data', train=True, download=True, transform=transform_mnist_usps)\n",
        "usps_test = USPS(root='./data', train=False, download=True, transform=transform_mnist_usps)\n",
        "\n",
        "# USPS dataloaders\n",
        "usps_train_loader = DataLoader(usps_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "usps_test_loader = DataLoader(usps_test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "# Loading in the SVHN dataset\n",
        "svhn_train = SVHN(root='./data', split='train', download=True, transform=transform_svhn)\n",
        "svhn_test = SVHN(root='./data', split='test', download=True, transform=transform_svhn)\n",
        "\n",
        "# SVHN dataloaders\n",
        "svhn_train_loader = DataLoader(svhn_train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "svhn_test_loader = DataLoader(svhn_test, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "# Print dataset sizes for verification\n",
        "print(f\"MNIST Train Size: {len(mnist_train)}\")\n",
        "print(f\"MNIST Test Size: {len(mnist_test)}\")\n",
        "print(f\"USPS Train Size: {len(usps_train)}\")\n",
        "print(f\"USPS Test Size: {len(usps_test)}\")\n",
        "print(f\"SVHN Train Size: {len(svhn_train)}\")\n",
        "print(f\"SVHN Test Size: {len(svhn_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xELoPpXN8-R",
        "outputId": "74631cd3-4326-45c5-86d5-c2d1aa187d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST Train Subset Size: 10000\n",
            "MNIST Test Subset Size: 5000\n",
            "USPS Train Subset Size: 5000\n",
            "USPS Test Subset Size: 1000\n",
            "SVHN Train Subset Size: 10000\n",
            "SVHN Test Subset Size: 5000\n"
          ]
        }
      ],
      "source": [
        "# Create subsets of the train and test datasets\n",
        "mnist_train_subset = create_subset(mnist_train, 10000)\n",
        "mnist_test_subset = create_subset(mnist_test, 5000)\n",
        "usps_train_subset = create_subset(usps_train, 5000)\n",
        "usps_test_subset = create_subset(usps_test, 1000)\n",
        "svhn_train_subset = create_subset(svhn_train, 10000)\n",
        "svhn_test_subset = create_subset(svhn_test, 5000)\n",
        "\n",
        "# Creating the subset dataloaders\n",
        "mnist_train_subset_loader = DataLoader(mnist_train_subset, batch_size=batch_size, shuffle=True)\n",
        "mnist_test_subset_loader = DataLoader(mnist_test_subset, batch_size=batch_size, shuffle=False)\n",
        "usps_train_subset_loader = DataLoader(usps_train_subset, batch_size=batch_size, shuffle=True)\n",
        "usps_test_subset_loader = DataLoader(usps_test_subset, batch_size=batch_size, shuffle=False)\n",
        "svhn_train_subset_loader = DataLoader(svhn_train_subset, batch_size=batch_size, shuffle=True)\n",
        "svhn_test_subset_loader = DataLoader(svhn_test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Check the size of the subsets\n",
        "print(f\"MNIST Train Subset Size: {len(mnist_train_subset)}\")\n",
        "print(f\"MNIST Test Subset Size: {len(mnist_test_subset)}\")\n",
        "print(f\"USPS Train Subset Size: {len(usps_train_subset)}\")\n",
        "print(f\"USPS Test Subset Size: {len(usps_test_subset)}\")\n",
        "print(f\"SVHN Train Subset Size: {len(svhn_train_subset)}\")\n",
        "print(f\"SVHN Test Subset Size: {len(svhn_test_subset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMYzFRdN8-R"
      },
      "source": [
        "## DANN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPP6fYv5N8-R"
      },
      "source": [
        "#### Util functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw9lRgA1N8-R"
      },
      "outputs": [],
      "source": [
        "class ReverseLayerF(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None\n",
        "\n",
        "def optimizer_scheduler(optimizer, p):\n",
        "    \"\"\"\n",
        "    Adjust the learning rate of optimizer\n",
        "    :param optimizer: optimizer for updating parameters\n",
        "    :param p: a variable for adjusting learning rate\n",
        "    :return: optimizer\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceBSIlsMN8-R"
      },
      "source": [
        "#### DANN Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWnx1O60N8-R",
        "outputId": "f081a13c-5e68-427b-ff82-c5c9e1e4006e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 176MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: torch.Size([64, 2048])\n",
            "Class output shape: torch.Size([64, 31])\n",
            "Domain output shape: torch.Size([64, 2])\n"
          ]
        }
      ],
      "source": [
        "# class DANN(nn.Module):\n",
        "#     def __init__(self, num_classes, alpha=1.0):\n",
        "#         super(DANN, self).__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.feature_extractor = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "#         num_features = self.feature_extractor.fc.in_features\n",
        "#         self.feature_extractor.fc = nn.Identity()\n",
        "#         self.class_classifier = nn.Sequential(\n",
        "#             nn.Linear(num_features, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(256, num_classes)\n",
        "#         )\n",
        "#         self.domain_classifier = nn.Sequential(\n",
        "#             nn.Linear(num_features, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(256, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x, alpha=None):\n",
        "#         if alpha is None:\n",
        "#             alpha = self.alpha\n",
        "#         features = self.feature_extractor(x)\n",
        "#         reverse_features = ReverseLayerF.apply(features, alpha)\n",
        "#         class_output = self.class_classifier(features)\n",
        "#         domain_output = self.domain_classifier(reverse_features)\n",
        "#         return class_output, domain_output\n",
        "\n",
        "\n",
        "# # Create the model\n",
        "# num_classes = 31\n",
        "# model = DANN(num_classes, 1.0).to(device)\n",
        "# summary(model, input_size=(64, 3, 224, 224))\n",
        "\n",
        "class Extractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Extractor, self).__init__()\n",
        "        self.feature_extractor = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "        num_features = self.feature_extractor.fc.in_features\n",
        "        self.feature_extractor.fc = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return features\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.class_classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        class_output = self.class_classifier(x)\n",
        "        return class_output\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, alpha):\n",
        "        reversed_input = ReverseLayerF.apply(x, alpha)\n",
        "        domain_output = self.domain_classifier(reversed_input)\n",
        "        return domain_output\n",
        "\n",
        "# Testing the flow of the model\n",
        "extractor = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "domain_classifier = DomainClassifier().to(device)\n",
        "\n",
        "# Testing the flow of the model\n",
        "x = torch.randn(64, 3, 224, 224).to(device)\n",
        "features = extractor(x)\n",
        "print(\"Feature shape:\", features.shape)\n",
        "\n",
        "class_output = classifier(features)\n",
        "print(\"Class output shape:\", class_output.shape)\n",
        "\n",
        "domain_output = domain_classifier(features, 1.0)\n",
        "print(\"Domain output shape:\", domain_output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBUwpH7RN8-S"
      },
      "source": [
        "#### Defining the training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEgKclHiN8-S"
      },
      "outputs": [],
      "source": [
        "# def train_dann_model(model, source_loader, target_loader, num_epochs, loss_class, loss_domain, optimizer, source_name, target_name, device):\n",
        "#     model.train()\n",
        "#     for epoch in range(num_epochs):\n",
        "#         total_loss = 0.0\n",
        "#         correct_class = 0\n",
        "#         correct_domain = 0\n",
        "#         total_samples = 0\n",
        "\n",
        "#         source_iter = iter(source_loader)\n",
        "#         target_iter = iter(target_loader)\n",
        "#         num_batches = min(len(source_iter), len(target_iter))\n",
        "\n",
        "#         with tqdm(total=num_batches, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
        "#             for _ in range(num_batches):\n",
        "#                 source_data, source_labels = next(source_iter)\n",
        "#                 target_data, _ = next(target_iter)\n",
        "\n",
        "#                 source_data, source_labels = source_data.to(device), source_labels.to(device)\n",
        "#                 target_data = target_data.to(device)\n",
        "\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#                 # Forward pass\n",
        "#                 class_output, domain_output = model(source_data)\n",
        "#                 _, target_domain_output = model(target_data)\n",
        "\n",
        "#                 # Compute losses\n",
        "#                 loss_s_label = loss_class(class_output, source_labels)\n",
        "#                 loss_s_domain = loss_domain(domain_output, torch.zeros_like(domain_output))\n",
        "#                 loss_t_domain = loss_domain(target_domain_output, torch.ones_like(target_domain_output))\n",
        "\n",
        "#                 loss = loss_s_label + loss_s_domain + loss_t_domain\n",
        "#                 total_loss += loss.item()\n",
        "\n",
        "#                 # Backward pass and optimization\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "\n",
        "#                 # Compute accuracies\n",
        "#                 _, predicted_class = torch.max(class_output, 1)\n",
        "#                 correct_class += (predicted_class == source_labels).sum().item()\n",
        "\n",
        "#                 _, predicted_domain = torch.max(domain_output, 1)\n",
        "#                 correct_domain += (predicted_domain == torch.zeros_like(domain_output)).sum().item()\n",
        "\n",
        "#                 _, predicted_target_domain = torch.max(target_domain_output, 1)\n",
        "#                 correct_domain += (predicted_target_domain == torch.ones_like(target_domain_output)).sum().item()\n",
        "\n",
        "#                 total_samples += source_labels.size(0)\n",
        "\n",
        "#                 pbar.set_postfix(loss=total_loss / total_samples, class_acc=correct_class / total_samples, domain_acc=correct_domain / (2 * total_samples))\n",
        "#                 pbar.update(1)\n",
        "\n",
        "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/total_samples:.4f}, Class Accuracy: {correct_class/total_samples:.4f}, Domain Accuracy: {correct_domain/(2*total_samples):.4f}\")\n",
        "\n",
        "def tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode, device):\n",
        "    encoder.to(device)\n",
        "    classifier.to(device)\n",
        "\n",
        "    # Set models to eval mode\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    if training_mode == 'DANN':\n",
        "        discriminator.to(device)\n",
        "        discriminator.eval()\n",
        "        domain_correct = 0\n",
        "\n",
        "    source_correct = 0\n",
        "    target_correct = 0\n",
        "\n",
        "    # Combine source and target test loaders with tqdm progress bar\n",
        "    total_batches = min(len(source_test_loader), len(target_test_loader))\n",
        "    with tqdm(total=total_batches, desc=\"Testing\", unit=\"batch\") as progress_bar:\n",
        "        for batch_idx, (source_data, target_data) in enumerate(zip(source_test_loader, target_test_loader)):\n",
        "            p = float(batch_idx) / total_batches\n",
        "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "            # Process source and target data\n",
        "            source_image, source_label = source_data\n",
        "            target_image, target_label = target_data\n",
        "\n",
        "            source_image, source_label = source_image.to(device), source_label.to(device)\n",
        "            target_image, target_label = target_image.to(device), target_label.to(device)\n",
        "\n",
        "            # Compute source and target predictions\n",
        "            source_pred = compute_output(encoder, classifier, source_image, alpha=None)\n",
        "            target_pred = compute_output(encoder, classifier, target_image, alpha=None)\n",
        "\n",
        "            # Update correct counts\n",
        "            source_correct += source_pred.eq(source_label.data.view_as(source_pred)).sum().item()\n",
        "            target_correct += target_pred.eq(target_label.data.view_as(target_pred)).sum().item()\n",
        "\n",
        "            if training_mode == 'DANN':\n",
        "                # Process combined images for domain classification\n",
        "                combined_image = torch.cat((source_image, target_image), 0)\n",
        "                domain_labels = torch.cat((torch.zeros(source_label.size(0), dtype=torch.long),\n",
        "                                           torch.ones(target_label.size(0), dtype=torch.long)), 0).to(device)\n",
        "\n",
        "                # Compute domain predictions\n",
        "                domain_pred = compute_output(encoder, discriminator, combined_image, alpha=alpha)\n",
        "                domain_correct += domain_pred.eq(domain_labels.data.view_as(domain_pred)).sum().item()\n",
        "\n",
        "            # Update tqdm progress bar\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    source_dataset_len = len(source_test_loader.dataset)\n",
        "    target_dataset_len = len(target_test_loader.dataset)\n",
        "\n",
        "    accuracies = {\n",
        "        \"Source\": {\n",
        "            \"correct\": source_correct,\n",
        "            \"total\": source_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(source_correct, source_dataset_len)\n",
        "        },\n",
        "        \"Target\": {\n",
        "            \"correct\": target_correct,\n",
        "            \"total\": target_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(target_correct, target_dataset_len)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if training_mode == 'DANN':\n",
        "        accuracies[\"Domain\"] = {\n",
        "            \"correct\": domain_correct,\n",
        "            \"total\": source_dataset_len + target_dataset_len,\n",
        "            \"accuracy\": calculate_accuracy(domain_correct, source_dataset_len + target_dataset_len)\n",
        "        }\n",
        "\n",
        "    print_accuracy(training_mode, accuracies)\n",
        "\n",
        "def compute_output(encoder, classifier, images, alpha=None):\n",
        "    features = encoder(images)\n",
        "    if isinstance(classifier, DomainClassifier):\n",
        "        outputs = classifier(features, alpha)  # Domain classifier\n",
        "    else:\n",
        "        outputs = classifier(features)  # Category classifier\n",
        "    preds = outputs.data.max(1, keepdim=True)[1]\n",
        "    return preds\n",
        "\n",
        "\n",
        "def calculate_accuracy(correct, total):\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "def print_accuracy(training_mode, accuracies):\n",
        "    print(f\"Test Results on {training_mode}:\")\n",
        "    for key, value in accuracies.items():\n",
        "        print(f\"{key} Accuracy: {value['correct']}/{value['total']} ({value['accuracy']:.2f}%)\")\n",
        "\n",
        "def train_dann(encoder, classifier, discriminator, source_train_loader, target_train_loader, epochs, device, source_test_loader, target_test_loader):\n",
        "    print(\"Training with the DANN adaptation method\")\n",
        "\n",
        "    classifier_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    discriminator_criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        list(encoder.parameters()) +\n",
        "        list(classifier.parameters()) +\n",
        "        list(discriminator.parameters()),\n",
        "        lr=0.01,\n",
        "        momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
        "\n",
        "        epoch_total_loss = 0.0\n",
        "        epoch_class_loss = 0.0\n",
        "        epoch_domain_loss = 0.0\n",
        "\n",
        "        # Setting the models to train mode\n",
        "        encoder.train()\n",
        "        classifier.train()\n",
        "        discriminator.train()\n",
        "\n",
        "        start_steps = epoch * len(source_train_loader)\n",
        "        total_steps = epochs * len(target_train_loader)\n",
        "\n",
        "        # Use tqdm to track the progress of the batch loop\n",
        "        progress_bar = tqdm(zip(source_train_loader, target_train_loader), total=min(len(source_train_loader), len(target_train_loader)), desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
        "\n",
        "        for batch_idx, (source_data, target_data) in enumerate(progress_bar):\n",
        "            source_image, source_label = source_data\n",
        "            target_image, target_label = target_data\n",
        "\n",
        "            p = float(batch_idx + start_steps) / total_steps\n",
        "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "            source_image, source_label = source_image.to(device), source_label.to(device)\n",
        "            target_image, target_label = target_image.to(device), target_label.to(device)\n",
        "\n",
        "            combined_image = torch.cat((source_image, target_image), 0)\n",
        "\n",
        "            optimizer = optimizer_scheduler(optimizer=optimizer, p=p)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            combined_feature = encoder(combined_image)\n",
        "            source_feature = encoder(source_image)\n",
        "\n",
        "            # 1. Classification loss\n",
        "            class_pred = classifier(source_feature)\n",
        "            class_loss = classifier_criterion(class_pred, source_label)\n",
        "\n",
        "            # 2. Domain loss\n",
        "            domain_pred = discriminator(combined_feature, alpha)\n",
        "\n",
        "            domain_source_labels = torch.zeros(source_label.shape[0]).type(torch.LongTensor).to(device)\n",
        "            domain_target_labels = torch.ones(target_label.shape[0]).type(torch.LongTensor).to(device)\n",
        "            domain_combined_label = torch.cat((domain_source_labels, domain_target_labels), 0)\n",
        "            domain_loss = discriminator_criterion(domain_pred, domain_combined_label)\n",
        "\n",
        "            total_loss = class_loss + domain_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_total_loss += total_loss.item()\n",
        "            epoch_class_loss += class_loss.item()\n",
        "            epoch_domain_loss += domain_loss.item()\n",
        "\n",
        "            # Update tqdm bar with loss information\n",
        "            progress_bar.set_postfix({\n",
        "                'Total Loss': f'{total_loss.item():.4f}',\n",
        "                'Class Loss': f'{class_loss.item():.4f}',\n",
        "                'Domain Loss': f'{domain_loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "            # End of epoch: Print average losses\n",
        "            avg_total_loss = epoch_total_loss / len(source_train_loader)\n",
        "            avg_class_loss = epoch_class_loss / len(source_train_loader)\n",
        "            avg_domain_loss = epoch_domain_loss / len(source_train_loader)\n",
        "\n",
        "            # print(f\"Epoch {epoch + 1}/{epochs} Summary:\")\n",
        "            # print(f\"  Average Total Loss: {avg_total_loss:.4f}\")\n",
        "            # print(f\"  Average Classification Loss: {avg_class_loss:.4f}\")\n",
        "            # print(f\"  Average Domain Loss: {avg_domain_loss:.4f}\")\n",
        "\n",
        "        # tester(encoder, classifier, discriminator, source_train_loader, target_train_loader, training_mode='DANN', device=device)\n",
        "        tester(encoder, classifier, discriminator, source_test_loader, target_test_loader, training_mode='DANN', device=device)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ypCMcFOGPX94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A -> W"
      ],
      "metadata": {
        "id": "LQyZu8N4kLzR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hTDovs8N8-S",
        "outputId": "d75f8b92-f121-4390-bd81-43b38e8d9b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 49/49 [01:04<00:00,  1.32s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 56/2837 (1.97%)\n",
            "Target Accuracy: 60/795 (7.55%)\n",
            "Domain Accuracy: 1343/3632 (36.98%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 49/49 [00:43<00:00,  1.13batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 69/2837 (2.43%)\n",
            "Target Accuracy: 70/795 (8.81%)\n",
            "Domain Accuracy: 1236/3632 (34.03%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 49/49 [00:23<00:00,  2.10batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 87/2837 (3.07%)\n",
            "Target Accuracy: 84/795 (10.57%)\n",
            "Domain Accuracy: 1273/3632 (35.05%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 49/49 [00:15<00:00,  3.13batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 83/2837 (2.93%)\n",
            "Target Accuracy: 66/795 (8.30%)\n",
            "Domain Accuracy: 932/3632 (25.66%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 49/49 [00:13<00:00,  3.66batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 70/2837 (2.47%)\n",
            "Target Accuracy: 58/795 (7.30%)\n",
            "Domain Accuracy: 824/3632 (22.69%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "amazon_loader = load_data(data_path, 'amazon', 16, 'src')\n",
        "webcam_loader = load_data(data_path, 'webcam', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# train_dann_model(model, amazon_loader, webcam_loader, 10, loss_class, loss_domain, optimizer, \"amazon\", \"webcam\", device)\n",
        "# train_dann_model(model, amazon_loader, webcam_loader, 5, loss_class, loss_domain, optimizer, \"amazon\", \"webcam\", device)\n",
        "train_dann(encoder, classifier, discriminator, amazon_loader, webcam_loader, 5, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A -> D"
      ],
      "metadata": {
        "id": "iCVjjbLCkPSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_loader = load_data(data_path, 'amazon', 16, 'src')\n",
        "dslr_loader = load_data(data_path, 'dslr', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, amazon_loader, dslr_loader, 5, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gz_s1bZkQZS",
        "outputId": "93fb5263-1d40-421a-d58b-e8dd12b3edf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 31/31 [00:12<00:00,  2.44batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 30/2837 (1.06%)\n",
            "Target Accuracy: 38/508 (7.48%)\n",
            "Domain Accuracy: 666/3345 (19.91%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:11<00:00,  2.65batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 36/2837 (1.27%)\n",
            "Target Accuracy: 38/508 (7.48%)\n",
            "Domain Accuracy: 701/3345 (20.96%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:20<00:00,  1.54batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 24/2837 (0.85%)\n",
            "Target Accuracy: 44/508 (8.66%)\n",
            "Domain Accuracy: 695/3345 (20.78%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:11<00:00,  2.59batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 35/2837 (1.23%)\n",
            "Target Accuracy: 42/508 (8.27%)\n",
            "Domain Accuracy: 760/3345 (22.72%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:15<00:00,  1.98batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 38/2837 (1.34%)\n",
            "Target Accuracy: 50/508 (9.84%)\n",
            "Domain Accuracy: 740/3345 (22.12%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### W -> A"
      ],
      "metadata": {
        "id": "Cn66qotMkcG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webcam_loader = load_data(data_path, 'webcam', 16, 'src')\n",
        "amazon_loader = load_data(data_path, 'amazon', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, webcam_loader, amazon_loader, 5, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fYq2TH4kdRU",
        "outputId": "88e581d6-6804-4032-b8d6-25ecd3c33771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 50/50 [00:12<00:00,  4.14batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 129/795 (16.23%)\n",
            "Target Accuracy: 85/2837 (3.00%)\n",
            "Domain Accuracy: 1359/3632 (37.42%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 50/50 [00:13<00:00,  3.63batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 148/795 (18.62%)\n",
            "Target Accuracy: 84/2837 (2.96%)\n",
            "Domain Accuracy: 1032/3632 (28.41%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 50/50 [00:13<00:00,  3.85batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 302/795 (37.99%)\n",
            "Target Accuracy: 269/2837 (9.48%)\n",
            "Domain Accuracy: 802/3632 (22.08%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 50/50 [00:13<00:00,  3.66batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 497/795 (62.52%)\n",
            "Target Accuracy: 391/2837 (13.78%)\n",
            "Domain Accuracy: 970/3632 (26.71%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 50/50 [00:13<00:00,  3.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 636/795 (80.00%)\n",
            "Target Accuracy: 457/2837 (16.11%)\n",
            "Domain Accuracy: 999/3632 (27.51%)\n",
            "Epoch 5/5 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### W -> D"
      ],
      "metadata": {
        "id": "UHNYNu5akmEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webcam_loader = load_data(data_path, 'webcam', 16, 'src')\n",
        "dslr_loader = load_data(data_path, 'dslr', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, webcam_loader, dslr_loader, 5, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zE4dbs7kpGy",
        "outputId": "24ac0b76-e91f-4d0f-d94e-8033e6b120ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 31/31 [00:11<00:00,  2.61batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 41/795 (5.16%)\n",
            "Target Accuracy: 24/508 (4.72%)\n",
            "Domain Accuracy: 521/1303 (39.98%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:16<00:00,  1.88batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 46/795 (5.79%)\n",
            "Target Accuracy: 25/508 (4.92%)\n",
            "Domain Accuracy: 677/1303 (51.96%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:10<00:00,  2.85batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 49/795 (6.16%)\n",
            "Target Accuracy: 26/508 (5.12%)\n",
            "Domain Accuracy: 747/1303 (57.33%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:16<00:00,  1.85batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 61/795 (7.67%)\n",
            "Target Accuracy: 33/508 (6.50%)\n",
            "Domain Accuracy: 741/1303 (56.87%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 31/31 [00:10<00:00,  2.86batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 67/795 (8.43%)\n",
            "Target Accuracy: 35/508 (6.89%)\n",
            "Domain Accuracy: 723/1303 (55.49%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D -> A"
      ],
      "metadata": {
        "id": "W0bBm9JDkub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dslr_loader = load_data(data_path, 'dslr', 16, 'src')\n",
        "amazon_loader = load_data(data_path, 'amazon', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, dslr_loader, amazon_loader, 5, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKjxAF-skuI5",
        "outputId": "6956edaf-4041-4be8-d6ed-b4f35b43289f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 32/32 [00:09<00:00,  3.31batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 37/508 (7.28%)\n",
            "Target Accuracy: 0/2837 (0.00%)\n",
            "Domain Accuracy: 940/3345 (28.10%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:13<00:00,  2.34batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 102/508 (20.08%)\n",
            "Target Accuracy: 72/2837 (2.54%)\n",
            "Domain Accuracy: 895/3345 (26.76%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:11<00:00,  2.79batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 151/508 (29.72%)\n",
            "Target Accuracy: 117/2837 (4.12%)\n",
            "Domain Accuracy: 778/3345 (23.26%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:12<00:00,  2.66batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 239/508 (47.05%)\n",
            "Target Accuracy: 154/2837 (5.43%)\n",
            "Domain Accuracy: 532/3345 (15.90%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:10<00:00,  2.96batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 364/508 (71.65%)\n",
            "Target Accuracy: 248/2837 (8.74%)\n",
            "Domain Accuracy: 518/3345 (15.49%)\n",
            "Epoch 5/5 completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D -> W"
      ],
      "metadata": {
        "id": "CRpGp0WnlLsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dslr_loader = load_data(data_path, 'dslr', 16, 'src')\n",
        "webcam_loader = load_data(data_path, 'webcam', 16, 'tar')\n",
        "\n",
        "print(\"Number of labels in Amazon dataset:\", len(amazon_loader.dataset.classes))\n",
        "\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(31).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, dslr_loader, webcam_loader, 5, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMmMlIsLlM9Q",
        "outputId": "92980c61-f02a-4cab-e6d2-b5d005c5a515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of labels in Amazon dataset: 31\n",
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 32/32 [00:12<00:00,  2.64batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 55/508 (10.83%)\n",
            "Target Accuracy: 45/795 (5.66%)\n",
            "Domain Accuracy: 751/1303 (57.64%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:12<00:00,  2.64batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 81/508 (15.94%)\n",
            "Target Accuracy: 63/795 (7.92%)\n",
            "Domain Accuracy: 819/1303 (62.85%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:12<00:00,  2.50batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 94/508 (18.50%)\n",
            "Target Accuracy: 74/795 (9.31%)\n",
            "Domain Accuracy: 803/1303 (61.63%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:12<00:00,  2.66batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 165/508 (32.48%)\n",
            "Target Accuracy: 117/795 (14.72%)\n",
            "Domain Accuracy: 727/1303 (55.79%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing: 100%|██████████| 32/32 [00:13<00:00,  2.31batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 184/508 (36.22%)\n",
            "Target Accuracy: 144/795 (18.11%)\n",
            "Domain Accuracy: 634/1303 (48.66%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Experiments"
      ],
      "metadata": {
        "id": "x0cwYWGyANHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### M -> U"
      ],
      "metadata": {
        "id": "MCtrg30iA3rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(10).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, mnist_train_subset_loader, usps_train_subset_loader, 5, device, mnist_test_subset_loader, usps_test_subset_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yylV0CvXAOxB",
        "outputId": "b5e745a5-5cee-4e3b-d2a4-84123573d534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 974/5000 (19.48%)\n",
            "Target Accuracy: 913/1000 (91.30%)\n",
            "Domain Accuracy: 839/6000 (13.98%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.91batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 991/5000 (19.82%)\n",
            "Target Accuracy: 943/1000 (94.30%)\n",
            "Domain Accuracy: 1125/6000 (18.75%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.94batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 992/5000 (19.84%)\n",
            "Target Accuracy: 937/1000 (93.70%)\n",
            "Domain Accuracy: 1006/6000 (16.77%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.96batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 992/5000 (19.84%)\n",
            "Target Accuracy: 954/1000 (95.40%)\n",
            "Domain Accuracy: 1161/6000 (19.35%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.99batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 993/5000 (19.86%)\n",
            "Target Accuracy: 932/1000 (93.20%)\n",
            "Domain Accuracy: 1073/6000 (17.88%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### U -> M"
      ],
      "metadata": {
        "id": "a_NtKEokCYC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(10).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, usps_train_subset_loader, mnist_train_subset_loader, 5, device, usps_test_subset_loader, mnist_test_subset_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TANJW8t_CXp6",
        "outputId": "f040f879-f828-434f-cb5d-eaa4f1f01375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.99batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 936/1000 (93.60%)\n",
            "Target Accuracy: 859/5000 (17.18%)\n",
            "Domain Accuracy: 1094/6000 (18.23%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.99batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 951/1000 (95.10%)\n",
            "Target Accuracy: 901/5000 (18.02%)\n",
            "Domain Accuracy: 1172/6000 (19.53%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.99batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 962/1000 (96.20%)\n",
            "Target Accuracy: 954/5000 (19.08%)\n",
            "Domain Accuracy: 1155/6000 (19.25%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.98batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 967/1000 (96.70%)\n",
            "Target Accuracy: 956/5000 (19.12%)\n",
            "Domain Accuracy: 1083/6000 (18.05%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 125/125 [00:15<00:00,  7.91batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 969/1000 (96.90%)\n",
            "Target Accuracy: 950/5000 (19.00%)\n",
            "Domain Accuracy: 1147/6000 (19.12%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### S -> M"
      ],
      "metadata": {
        "id": "k72DPUwqCfSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "# Create the model\n",
        "encoder = Extractor().to(device)\n",
        "classifier = Classifier(10).to(device)\n",
        "discriminator = DomainClassifier().to(device)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_dann(encoder, classifier, discriminator, svhn_train_subset_loader, mnist_train_subset_loader, 5, device, svhn_test_subset_loader, mnist_test_subset_loader)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdWfSN9fChEd",
        "outputId": "c5dd3936-ab72-447e-955a-f0688df7753e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with the DANN adaptation method\n",
            "Epoch: 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 625/625 [01:17<00:00,  8.03batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 4020/5000 (80.40%)\n",
            "Target Accuracy: 3198/5000 (63.96%)\n",
            "Domain Accuracy: 4683/10000 (46.83%)\n",
            "Epoch 1/5 completed.\n",
            "Epoch: 2/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 625/625 [01:18<00:00,  8.00batch/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 4498/5000 (89.96%)\n",
            "Target Accuracy: 3595/5000 (71.90%)\n",
            "Domain Accuracy: 6868/10000 (68.68%)\n",
            "Epoch 2/5 completed.\n",
            "Epoch: 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 625/625 [01:18<00:00,  7.97batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 4511/5000 (90.22%)\n",
            "Target Accuracy: 3719/5000 (74.38%)\n",
            "Domain Accuracy: 5182/10000 (51.82%)\n",
            "Epoch 3/5 completed.\n",
            "Epoch: 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 625/625 [01:18<00:00,  7.92batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 4631/5000 (92.62%)\n",
            "Target Accuracy: 3889/5000 (77.78%)\n",
            "Domain Accuracy: 4824/10000 (48.24%)\n",
            "Epoch 4/5 completed.\n",
            "Epoch: 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 625/625 [01:18<00:00,  7.92batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results on DANN:\n",
            "Source Accuracy: 4636/5000 (92.72%)\n",
            "Target Accuracy: 3755/5000 (75.10%)\n",
            "Domain Accuracy: 4955/10000 (49.55%)\n",
            "Epoch 5/5 completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}